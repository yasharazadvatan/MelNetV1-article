{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b53fcb-844c-4402-a9a0-5270dcd6bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6600ef-884f-4cd4-966d-b016ededb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './dataset/train/images/'\n",
    "train_lbl_path = './dataset/train/labels/'\n",
    "\n",
    "valid_data_path = './dataset/valid/images/'\n",
    "valid_lbl_path = './dataset/valid/labels/'\n",
    "\n",
    "test_data_path = './dataset/test/images/'\n",
    "\n",
    "# KITTI Dataset\n",
    "CLASSES = [ \n",
    "    'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 32\n",
    "RESIZE_TO = 640\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CONF_THRESHOLD = 0.5\n",
    "MAP_IOU_THRESH = 0.5\n",
    "NMS_IOU_THRESH = 0.45\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "\n",
    "S = [RESIZE_TO // 32, RESIZE_TO // 16]\n",
    "\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "]\n",
    "\n",
    "scale = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0858a-6a39-4c4d-94d2-0006cdb8c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad399ff0-5ac0-4452-9ee2-4bbc701a7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(RESIZE_TO * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(RESIZE_TO * scale),\n",
    "            min_width=int(RESIZE_TO * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=RESIZE_TO, height=RESIZE_TO),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                )\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=RESIZE_TO),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=RESIZE_TO, min_width=RESIZE_TO, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e15a1-7de5-4a7a-82d9-2cfac7ce06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "\"B\" indicating a residual block\n",
    "\"S\" is for scale prediction block\n",
    "\"U\" is for upsampling the feature map\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 4],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 6],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 6],\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, 3 * (num_classes + 5), bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "class MelNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels=in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2))\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    model = MelNet(num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c15654-9aa2-4245-9b48-309a111b49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    boxes1 (tensor): width and height of the first bounding boxes\n",
    "    boxes2 (tensor): width and height of the second bounding boxes\n",
    "Returns:\n",
    "    tensor: IOU\n",
    "\"\"\"\n",
    "def iou_width_height(boxes1, boxes2):\n",
    "\n",
    "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
    "        boxes1[..., 1], boxes2[..., 1]\n",
    "    )\n",
    "    union = (\n",
    "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
    "    )\n",
    "    return intersection / union\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    boxes_preds (tensor): Predictions of Bounding Boxes\n",
    "    boxes_labels (tensor): Correct labels of Bounding Boxes\n",
    "Returns:\n",
    "    tensor: IOU\n",
    "\"\"\"\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "    specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "    iou_threshold (float)\n",
    "    threshold (float)\n",
    "Returns:\n",
    "    list: bboxes after performing NMS\n",
    "\"\"\"\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "    \n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0) \n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ] \n",
    "        \n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "         \n",
    "    return bboxes_after_nms\n",
    "    \n",
    "\"\"\"\n",
    "Parameters:\n",
    "    pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "    specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "    true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "    iou_threshold (float): threshold where predicted bboxes is correct\n",
    "Returns:\n",
    "    mAP value\n",
    "\"\"\"\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes,epoch, num_classes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "):\n",
    "    classes_precisions = []\n",
    "\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "                \n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        \n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "                    \n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        \n",
    "        classes_precisions.append(torch.trapz(precisions, recalls))\n",
    "    \n",
    "    return classes_precisions\n",
    "\n",
    "def get_evaluation_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    anchors,\n",
    "    threshold,\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "     \n",
    "    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        bboxes = [[] for _ in range(batch_size)]\n",
    "        for i in range(2):\n",
    "            S = predictions[i].shape[2]\n",
    "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
    "            boxes_scale_i = cells_to_bboxes(\n",
    "                predictions[i], anchor, S=S, is_preds=True\n",
    "            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx] += box\n",
    " \n",
    "        true_bboxes = cells_to_bboxes(\n",
    "            labels[1], anchor, S=S, is_preds=False\n",
    "        ) \n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            ) \n",
    "            \n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "          \n",
    "            for box in true_bboxes[idx]:\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "          \n",
    "            train_idx += 1\n",
    "    \n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Scales the predictions coming from the model to\n",
    "be relative to the entire image such that they for example later\n",
    "can be plotted or.\n",
    "INPUT:\n",
    "predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
    "anchors: the anchors used for the predictions\n",
    "S: the number of cells the image is divided in on the width (and height)\n",
    "is_preds: whether the input is predictions or the true bounding boxes\n",
    "OUTPUT:\n",
    "converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n",
    "                  object score, bounding box coordinates\n",
    "\"\"\"\n",
    "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
    "    BATCH_SIZE = predictions.shape[0]\n",
    "    num_anchors = len(anchors)\n",
    "    box_predictions = predictions[..., 1:5]\n",
    "    if is_preds:\n",
    "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
    "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
    "        scores = torch.sigmoid(predictions[..., 0:1])\n",
    "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
    "    else:\n",
    "        scores = predictions[..., 0:1]\n",
    "        best_class = predictions[..., 5:6]\n",
    "\n",
    "    cell_indices = (\n",
    "        torch.arange(S)\n",
    "        .repeat(predictions.shape[0], 3, S, 1)\n",
    "        .unsqueeze(-1)\n",
    "        .to(predictions.device)\n",
    "    )\n",
    "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
    "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
    "    w_h = 1 / S * box_predictions[..., 2:4]\n",
    "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
    "    return converted_bboxes.tolist()\n",
    "\n",
    "def check_class_accuracy(model, loader, epoch, threshold):\n",
    "    model.eval()\n",
    "    tot_class_preds, correct_class = 0, 0\n",
    "    tot_noobj, correct_noobj = 0, 0\n",
    "    tot_obj, correct_obj = 0, 0\n",
    "\n",
    "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
    "        x = x.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "\n",
    "        for i in range(2):\n",
    "            y[i] = y[i].to(DEVICE)\n",
    "            obj = y[i][..., 0] == 1 \n",
    "            noobj = y[i][..., 0] == 0 \n",
    "\n",
    "            correct_class += torch.sum(\n",
    "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
    "            )\n",
    "            \n",
    "            tot_class_preds += torch.sum(obj)\n",
    "\n",
    "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
    "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
    "            tot_obj += torch.sum(obj)\n",
    "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
    "            tot_noobj += torch.sum(noobj)\n",
    "            \n",
    "    class_acc = (correct_class/(tot_class_preds+1e-16))*100\n",
    "    no_obj_acc = (correct_noobj/(tot_noobj+1e-16))*100\n",
    "    obj_acc = (correct_obj/(tot_obj+1e-16))*100\n",
    "    writer.add_scalar('Class Accuracy/train', class_acc, epoch)\n",
    "    writer.add_scalar('No Obj Accuracy/train', no_obj_acc, epoch)\n",
    "    writer.add_scalar('Obj Accuracy/train', obj_acc, epoch)\n",
    "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
    "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
    "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in tqdm(loader):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"./checkpoint/my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=confiDEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def get_loaders():\n",
    "    \n",
    "    train_dataset = MelNetDataset(\n",
    "        transforms=train_transforms,\n",
    "        S=[RESIZE_TO // 32, RESIZE_TO // 16],\n",
    "        img_dir=train_data_path,\n",
    "        label_dir=train_lbl_path,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "     \n",
    "    test_dataset = MelNetDataset(\n",
    "        transforms=test_transforms,\n",
    "        S=[RESIZE_TO // 32, RESIZE_TO // 16],\n",
    "        img_dir=valid_data_path,\n",
    "        label_dir=valid_lbl_path,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    train_eval_dataset = MelNetDataset(\n",
    "        transforms=test_transforms,\n",
    "        S=[RESIZE_TO // 32, RESIZE_TO // 16],\n",
    "        img_dir=test_data_path,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "    \n",
    "    train_eval_loader = DataLoader(\n",
    "        dataset=train_eval_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, train_eval_loader\n",
    "\n",
    "def plot_couple_examples(model, loader, thresh, iou_thresh, anchors):\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        bboxes = [[] for _ in range(x.shape[0])]\n",
    "        for i in range(2):\n",
    "            batch_size, A, S, _, _ = out[i].shape\n",
    "            anchor = anchors[i]\n",
    "            boxes_scale_i = cells_to_bboxes(\n",
    "                out[i], anchor, S=S, is_preds=True\n",
    "            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx] += box\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        nms_boxes = non_max_suppression(\n",
    "            bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
    "        )\n",
    "        plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cd85c-a876-4785-9df1-e9e3bb5a8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    class_labels = CLASSES\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    ax.imshow(im)\n",
    "\n",
    "    for box in boxes:\n",
    "        class_pred = box[0] \n",
    "        box = box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[int(class_pred)],\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            upper_left_x * width,\n",
    "            upper_left_y * height,\n",
    "            s=class_labels[int(class_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865064d1-249e-409e-9243-455746f4b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelNetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, img_dir, label_dir, anchors, S = [13, 26], C = 20, transforms=None\n",
    "    ):\n",
    "        self.all_images = glob.glob(f\"{img_dir}/*\")       \n",
    "        self.labels_path = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 2\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.all_images[index]\n",
    "        image_name = image_path.split(os.path.sep)[-1]\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "        lbl_filename = image_name[:-4] + '.txt'\n",
    "        lbl_path = os.path.join(self.labels_path, lbl_filename)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []  \n",
    "        box_label = []\n",
    "         \n",
    "        with open(lbl_path, 'r') as f:\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                lbl = label_line.split(' ')\n",
    "                xc = float(lbl[1]) \n",
    "                yc = float(lbl[2]) \n",
    "                w = float(lbl[3]) \n",
    "                h = float(lbl[4])  \n",
    "                box_label.append([xc, yc, w, h, float(lbl[0])]) \n",
    "                \n",
    "        box_label = np.array(box_label)      \n",
    "        \n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image = image, bboxes = box_label)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "            \n",
    "        targets = [torch.zeros((self.num_anchors // 2, S, S, 6)) for S in self.S] \n",
    "        \n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending = True, dim = 0)\n",
    "            \n",
    "            x, y, width, height, class_label = box\n",
    "            \n",
    "            has_anchor = [False] * 2\n",
    "            \n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                \n",
    "                S = self.S[scale_idx]\n",
    "                \n",
    "                i, j = int(S * y), int(S * x)\n",
    "                \n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                \n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    \n",
    "                    x_cell, y_cell = S * x - j, S * y - i\n",
    "                    \n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )\n",
    "                    \n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    \n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "        return image, tuple(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ccd3-24b3-4ede-a629-dbafe49ece32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelNetLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, target, anchors):\n",
    "        obj = target[..., 0] == 1 \n",
    "        noobj = target[..., 0] == 0 \n",
    "        \n",
    "        # NO OBJECT LOSS \n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # OBJECT LOSS\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
    "        \n",
    "        # BOX COORDINATES\n",
    "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])\n",
    "        target[..., 3:5] = torch.log(\n",
    "            (1e-16 + target[..., 3:5] / anchors)\n",
    "        )\n",
    "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # CLASS LOSS\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c822ec-7742-4833-a250-d62fc996daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n",
    "        self.matrix = np.zeros((nc + 1, nc + 1))\n",
    "        self.nc = nc\n",
    "        self.conf = conf\n",
    "        self.iou_thres = iou_thres\n",
    "\n",
    "    def process_batch(self, detections, labels):\n",
    "        if detections is None:\n",
    "            gt_classes = labels.int()\n",
    "            for gc in gt_classes:\n",
    "                self.matrix[self.nc, gc] += 1\n",
    "            return\n",
    "\n",
    "        detections = detections[detections[:, 4] > self.conf]\n",
    "        gt_classes = labels[:, 0].int()\n",
    "        detection_classes = detections[:, 5].int()\n",
    "        iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "\n",
    "        x = torch.where(iou > self.iou_thres)\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        else:\n",
    "            matches = np.zeros((0, 3))\n",
    "\n",
    "        n = matches.shape[0] > 0\n",
    "        m0, m1, _ = matches.transpose().astype(int)\n",
    "        for i, gc in enumerate(gt_classes):\n",
    "            j = m0 == i\n",
    "            if n and sum(j) == 1:\n",
    "                self.matrix[detection_classes[m1[j]], gc] += 1\n",
    "            else:\n",
    "                self.matrix[self.nc, gc] += 1\n",
    "\n",
    "        if n:\n",
    "            for i, dc in enumerate(detection_classes):\n",
    "                if not any(m1 == i):\n",
    "                    self.matrix[dc, self.nc] += 1\n",
    "\n",
    "    def tp_fp(self):\n",
    "        tp = self.matrix.diagonal() \n",
    "        fp = self.matrix.sum(1) - tp\n",
    "       \n",
    "        return tp[:-1], fp[:-1]\n",
    "\n",
    "    def plot(self, normalize=True, save_dir='', names=()):\n",
    "        import seaborn as sn\n",
    "\n",
    "        array = self.matrix / ((self.matrix.sum(0).reshape(1, -1) + 1E-9) if normalize else 1)\n",
    "        array[array < 0.005] = np.nan \n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\n",
    "        nc, nn = self.nc, len(names)\n",
    "        sn.set(font_scale=1.0 if nc < 50 else 0.8)\n",
    "        labels = (0 < nn < 99) and (nn == nc)\n",
    "        ticklabels = (names + ['background']) if labels else 'auto'\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            sn.heatmap(array,\n",
    "                       ax=ax,\n",
    "                       annot=nc < 30,\n",
    "                       annot_kws={\n",
    "                           'size': 8},\n",
    "                       cmap='Blues',\n",
    "                       fmt='.2f',\n",
    "                       square=True,\n",
    "                       vmin=0.0,\n",
    "                       xticklabels=ticklabels,\n",
    "                       yticklabels=ticklabels).set_facecolor((1, 1, 1))\n",
    "        ax.set_xlabel('True')\n",
    "        ax.set_ylabel('Predicted')\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        fig.savefig(Path(save_dir) / 'confusion_matrix.png', dpi=250)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def print(self):\n",
    "        for i in range(self.nc + 1):\n",
    "            print(' '.join(map(str, self.matrix[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67e98a-e3a6-4346-9fcb-591d57f169e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\"\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, epoch, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y0, y1 = (\n",
    "            y[0].to(DEVICE),\n",
    "            y[1].to(DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n",
    "        \n",
    "    writer.add_scalar('Loss/train', mean_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = MelNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = MelNetLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_loader, test_loader, train_eval_loader = get_loaders()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_FILE, model, epoch, optimizer, LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(ANCHORS)\n",
    "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        current_epoch = epoch + 1\n",
    "        print(f\"Currently epoch {current_epoch}\")\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    " \n",
    "        if current_epoch > 1 and current_epoch % 25 == 0:\n",
    "            check_class_accuracy(model, test_loader, epoch, threshold=CONF_THRESHOLD)\n",
    "            \n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=NMS_IOU_THRESH,\n",
    "                anchors=ANCHORS,\n",
    "                threshold=CONF_THRESHOLD,\n",
    "            )\n",
    "            \n",
    "            all_classes_average_precisions = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                epoch,\n",
    "                iou_threshold=MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=NUM_CLASSES\n",
    "            )\n",
    "                       \n",
    "            for i in range(NUM_CLASSES):\n",
    "                print('class {}: mAP value is: {}'.format(CLASSES[i], all_classes_average_precisions[i]))\n",
    "                writer.add_scalar('mAP_{}/train'.format(CLASSES[i]), all_classes_average_precisions[i], epoch)\n",
    "                \n",
    "            mapval = sum(all_classes_average_precisions) / len(all_classes_average_precisions)\n",
    "            print(f\"mAP: {mapval.item()}\")\n",
    "            writer.add_scalar('mAP_50/train_50', mapval.item(), epoch)\n",
    "            \n",
    "            writer.add_scalars('mAP/train', {'{}'.format(CLASSES[0]):all_classes_average_precisions[0],\n",
    "                                             '{}'.format(CLASSES[1]):all_classes_average_precisions[1],\n",
    "                                             '{}'.format(CLASSES[2]):all_classes_average_precisions[2],\n",
    "                                             '{}'.format(CLASSES[3]):all_classes_average_precisions[3],\n",
    "                                             '{}'.format(CLASSES[4]):all_classes_average_precisions[4],\n",
    "                                             '{}'.format(CLASSES[5]):all_classes_average_precisions[5],\n",
    "                                             '{}'.format(CLASSES[6]):all_classes_average_precisions[6],\n",
    "                                             '{}'.format(CLASSES[7]):all_classes_average_precisions[7],\n",
    "                                             '{}'.format(CLASSES[8]):all_classes_average_precisions[8],\n",
    "                                             'all': mapval.item()\n",
    "                                              } , epoch) \n",
    "            \n",
    "            \n",
    "            model.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0379b-8449-4601-97d3-7afed06b1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
